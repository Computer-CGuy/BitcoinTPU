{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bitcoin TPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOAVFkXVyFy+FqvXHUs8Ccq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Computer-CGuy/BitcoinTPU/blob/main/Bitcoin_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW4hLSBr95Q0"
      },
      "source": [
        "!pip install --quiet cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OLuq7txGM2v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b029a5f-8ff4-4b52-968c-3b1f3c3e69a0"
      },
      "source": [
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:TPU has started up successfully with version pytorch-1.8\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XA6m0MSGFTG",
        "outputId": "079c4077-780c-47b1-89bf-516cca8c9025"
      },
      "source": [
        "CORES = len(torch_xla.core.xla_model.get_xla_supported_devices())\n",
        "print(\"Number of cores\",CORES)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of cores 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDyM07i19a9D"
      },
      "source": [
        "import time\n",
        "\n",
        "# from utils import SHA256_hash\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# device = xm.xla_device()\n",
        "# device = torch.device(\"cuda\")\n",
        "F32 = 0xffffffff\n",
        "base = [2863311530, 898303315, 1398735129, 0, 2147483648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 640, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "h = [0x6a09e667, 0xbb67ae85, 0x3c6ef372, 0xa54ff53a,0x510e527f, 0x9b05688c, 0x1f83d9ab, 0x5be0cd19]\n",
        "k = torch.LongTensor(\n",
        "    [0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5,\n",
        "      0x3956c25b, 0x59f111f1, 0x923f82a4, 0xab1c5ed5,\n",
        "      0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3,\n",
        "      0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174,\n",
        "      0xe49b69c1, 0xefbe4786, 0x0fc19dc6, 0x240ca1cc,\n",
        "      0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,\n",
        "      0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7,\n",
        "      0xc6e00bf3, 0xd5a79147, 0x06ca6351, 0x14292967,\n",
        "      0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13,\n",
        "      0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85,\n",
        "      0xa2bfe8a1, 0xa81a664b, 0xc24b8b70, 0xc76c51a3,\n",
        "      0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,\n",
        "      0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5,\n",
        "      0x391c0cb3, 0x4ed8aa4a, 0x5b9cca4f, 0x682e6ff3,\n",
        "      0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208,\n",
        "      0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2])#.to(device)\n",
        "# consts = [[7,25],[18,14],[3,29],[17,8],[19,13],[2,30],[13,19],[22,10],[6,26],[11,21],[25,7]]\n",
        "consts = [[2**7,2**25],[2**18,2**14],[2**3,2**29],[2**17,2**8],[2**19,2**13],[2**2,2**30],[2**13,2**19],[2**22,2**10],[2**6,2**26],[2**11,2**21],[25,2**7]]\n",
        "consts = torch.LongTensor(consts)#.to(device)\n",
        "class hashing_algorithm(Dataset):\n",
        "    def __init__(self,start,end,base):\n",
        "        self.base = base\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "    def __len__(self):\n",
        "        return self.end-self.start\n",
        "    def __getitem__(self,idx):\n",
        "        w = self.base\n",
        "        w[3] = idx+self.start\n",
        "        return torch.LongTensor(w),torch.LongTensor(h),torch.LongTensor([1]),torch.LongTensor([1])\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHgajny3ZxCM"
      },
      "source": [
        "def my_stack(batch):\n",
        "    # print(len(batch))\n",
        "    elem = batch[0]\n",
        "    # print(len(elem))\n",
        "    elem_type = type(elem)\n",
        "    # print(elem_type)\n",
        "    out = None\n",
        "    numel = sum([x.numel() for x in batch])\n",
        "    storage = elem.storage()._new_shared(numel)\n",
        "    out = elem.new(storage)\n",
        "    return torch.stack(batch, 1, out=out)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_TnBNd8VqpF"
      },
      "source": [
        "def my_collate(batch):\n",
        "    return [(*(my_stack(samples) for samples in zip(*batch)))]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX8TelDjimkW",
        "outputId": "00c38bf9-aa44-4469-b262-9f886ebd34e5"
      },
      "source": [
        "consts[3][0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gybPNDMVwXmK"
      },
      "source": [
        "import torch\n",
        "# F32 = 0xffffffff\n",
        "\n",
        "def _rotr(x, const,F32):\n",
        "    return ((x >> const[0]) | (x << (const[1]))) & F32\n",
        "\n",
        "def _maj(x, y, z):\n",
        "    return (x & y) ^ (x & z) ^ (y & z)\n",
        "\n",
        "def _ch(x, y, z):\n",
        "    return (x & y) ^ ((~x) & z)\n",
        "\n",
        "\n",
        "def SHA256_hash(w,a,b,c,d,e,f,g,h,k,t1,t2,F32,consts):\n",
        "    for i in range(16, 64):\n",
        "        torch.bitwise_xor(_rotr(w[i-15], consts[0],F32), _rotr(w[i-15], consts[1],F32),out=t1)\n",
        "        torch.bitwise_xor(t1,_rotr(w[i-15], consts[2],F32),out=t1)\n",
        "\n",
        "        # s1 \n",
        "\n",
        "        torch.bitwise_xor(_rotr(w[i-2], consts[3],F32),_rotr(w[i-2], consts[4],F32),out=t2)\n",
        "        torch.bitwise_xor(t2,_rotr(w[i-15], consts[3],F32),out=t2)\n",
        "        # s1[:] = _rotr(w[i-2], 17) ^ _rotr(w[i-2], 19) ^ _rotr(w[i-15], 10)\n",
        "        # w[i] = (w[i-16] + t1 + w[i-7] + t2) & F32\n",
        "        w[i] = (w[i-16] + t1 + w[i-7] + t2) & F32\n",
        "    for i in range(64): \n",
        "        torch.bitwise_xor(_rotr(a, consts[5],F32), _rotr(a, consts[6],F32),out=t2)\n",
        "        torch.bitwise_xor(t2,_rotr(a, consts[7],F32),out=t2)\n",
        "\n",
        "        # s0[:] = _rotr(chrs[i%8], 2) ^ _rotr(chrs[i%8], 13) ^ _rotr(chrs[i%8], 22)\n",
        "        torch.add(t2 ,_maj(a, b, c),out=t2)\n",
        "\n",
        "\n",
        "        torch.bitwise_xor(_rotr(e, consts[8],F32), _rotr(e, consts[9],F32),out=t1)\n",
        "        torch.bitwise_xor(t1,_rotr(e, consts[10],F32),out=t1)\n",
        "        \n",
        "        # s1[:] = _rotr(chrs[(i+4)%8], 6) ^ _rotr(chrs[(i+4)%8], 11) ^ _rotr(chrs[(i+4)%8], 25)\n",
        "        torch.add(h,t1,out=t1)\n",
        "        torch.add(_ch(e,f,g) + k[i],t1+w[i],out=t1)\n",
        "        # t1[:] = chrs[(i+7)%8] + s1 + _ch(chrs[(i+4)%8], chrs[(i+5)%8], chrs[(i+6)%8]) + k[i] + w[i]\n",
        "        \n",
        "        h = g\n",
        "        g = f\n",
        "        f = e\n",
        "        torch.add(d, t1,out=e)\n",
        "        torch.bitwise_and(e, F32,out=e)\n",
        "        d = c\n",
        "        c = b\n",
        "        b = a\n",
        "        torch.add(t1, t2,out=a)\n",
        "        torch.bitwise_and(a, F32,out=a)\n",
        "        # a = (t1 + t2) & F32"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZVpQ2-Wf872"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9TGzQS9JIsL"
      },
      "source": [
        "# hash_dataset = hashing_algorithm(0*batch_n, (0+1)*batch_n, base)\n",
        "# hash_dataloader = DataLoader(hash_dataset, batch_size=each, shuffle=False,collate_fn=my_collate)\n",
        "# mp_device_loader = pl.MpDeviceLoader(hash_dataloader, device)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynwR2HyRJQ9b"
      },
      "source": [
        "# for x in mp_device_loader:\n",
        "#     break"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXdZtOURJUFu"
      },
      "source": [
        "# len(x)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vB6ByY5YWGZ"
      },
      "source": [
        "F32=0xffffffff"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nbhm2Y24YYPx",
        "outputId": "27712fc1-b781-49bb-a4f2-7725cf9bbc23"
      },
      "source": [
        "import torch_xla.debug.metrics as met\n",
        "\n",
        "print(met.metrics_report())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Metric: XrtAllocateFromTensor\n",
            "  TotalSamples: 32\n",
            "  Accumulator: 01s198ms229.243us\n",
            "  Mean: 037ms444.664us\n",
            "  StdDev: 170ms368.075us\n",
            "  Rate: 0.259621 / second\n",
            "  Percentiles: 25%=528.957us; 50%=789.428us; 80%=009ms102.109us; 90%=012ms705.927us; 95%=098ms099.853us; 99%=981ms541.900us\n",
            "Metric: XrtCompile\n",
            "  TotalSamples: 11\n",
            "  Accumulator: 02s852ms934.014us\n",
            "  Mean: 168ms357.638us\n",
            "  StdDev: 489ms211.989us\n",
            "  Rate: 5.73993 / second\n",
            "  Percentiles: 25%=013ms146.591us; 50%=014ms853.015us; 80%=014ms126.024us; 90%=015ms688.174us; 95%=02s715ms380.815us; 99%=02s715ms380.815us\n",
            "Metric: XrtExecute\n",
            "  TotalSamples: 2016\n",
            "  Accumulator: 02s480ms203.577us\n",
            "  Mean: 001ms250.453us\n",
            "  StdDev: 229.743us\n",
            "  Rate: 49.9536 / second\n",
            "  Percentiles: 25%=001ms094.586us; 50%=001ms232.074us; 80%=001ms402.427us; 90%=002ms559.056us; 95%=002ms722.613us; 99%=002ms878.421us\n",
            "Metric: XrtExecutorEvict\n",
            "  TotalSamples: 0\n",
            "  Accumulator: nanB\n",
            "  Mean: nanB\n",
            "  StdDev: nanB\n",
            "  Percentiles: \n",
            "Metric: XrtReadLiteral\n",
            "  TotalSamples: 4032\n",
            "  Accumulator: 02s130ms896.834us\n",
            "  Mean: 527.378us\n",
            "  StdDev: 135.786us\n",
            "  Rate: 367.658 / second\n",
            "  Percentiles: 25%=417.887us; 50%=526.617us; 80%=634.401us; 90%=685.312us; 95%=777.989us; 99%=884.628us\n",
            "Metric: XrtReleaseAllocation\n",
            "  TotalSamples: 2895\n",
            "  Accumulator: 097ms360.722us\n",
            "  Mean: 035.784us\n",
            "  StdDev: 024.913us\n",
            "  Rate: 11.1821 / second\n",
            "  Percentiles: 25%=025.764us; 50%=031.376us; 80%=044.337us; 90%=060.907us; 95%=084.130us; 99%=125.112us\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RkkptwZbhxf",
        "outputId": "7b803938-ee33-4503-fd7c-caae205a20e4"
      },
      "source": [
        "import torch_xla.debug.metrics as met\n",
        "\n",
        "print(met.metrics_report())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Metric: XrtAllocateFromTensor\n",
            "  TotalSamples: 32\n",
            "  Accumulator: 01s198ms229.243us\n",
            "  Mean: 037ms444.664us\n",
            "  StdDev: 170ms368.075us\n",
            "  Rate: 0.259621 / second\n",
            "  Percentiles: 25%=528.957us; 50%=789.428us; 80%=009ms102.109us; 90%=012ms705.927us; 95%=098ms099.853us; 99%=981ms541.900us\n",
            "Metric: XrtCompile\n",
            "  TotalSamples: 11\n",
            "  Accumulator: 02s852ms934.014us\n",
            "  Mean: 168ms357.638us\n",
            "  StdDev: 489ms211.989us\n",
            "  Rate: 5.73993 / second\n",
            "  Percentiles: 25%=013ms146.591us; 50%=014ms853.015us; 80%=014ms126.024us; 90%=015ms688.174us; 95%=02s715ms380.815us; 99%=02s715ms380.815us\n",
            "Metric: XrtExecute\n",
            "  TotalSamples: 2016\n",
            "  Accumulator: 02s480ms203.577us\n",
            "  Mean: 001ms250.453us\n",
            "  StdDev: 229.743us\n",
            "  Rate: 49.9536 / second\n",
            "  Percentiles: 25%=001ms094.586us; 50%=001ms232.074us; 80%=001ms402.427us; 90%=002ms559.056us; 95%=002ms722.613us; 99%=002ms878.421us\n",
            "Metric: XrtExecutorEvict\n",
            "  TotalSamples: 0\n",
            "  Accumulator: nanB\n",
            "  Mean: nanB\n",
            "  StdDev: nanB\n",
            "  Percentiles: \n",
            "Metric: XrtReadLiteral\n",
            "  TotalSamples: 4032\n",
            "  Accumulator: 02s130ms896.834us\n",
            "  Mean: 527.378us\n",
            "  StdDev: 135.786us\n",
            "  Rate: 367.658 / second\n",
            "  Percentiles: 25%=417.887us; 50%=526.617us; 80%=634.401us; 90%=685.312us; 95%=777.989us; 99%=884.628us\n",
            "Metric: XrtReleaseAllocation\n",
            "  TotalSamples: 2895\n",
            "  Accumulator: 097ms360.722us\n",
            "  Mean: 035.784us\n",
            "  StdDev: 024.913us\n",
            "  Rate: 11.1821 / second\n",
            "  Percentiles: 25%=025.764us; 50%=031.376us; 80%=044.337us; 90%=060.907us; 95%=084.130us; 99%=125.112us\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq3_Q5qNdQj9",
        "outputId": "5fa9f767-d475-4ffd-c361-a05c511a6530"
      },
      "source": [
        "def _mp_fn(index):\n",
        "    global batch_n\n",
        "    global each\n",
        "    global start\n",
        "    global k\n",
        "    global consts\n",
        "    print(\"Started\",index)\n",
        "    device = xm.xla_device()\n",
        "    k = k.to(device)\n",
        "    consts = consts.to(device)\n",
        "    print(\"Datak\")\n",
        "    datak = torch.zeros([64,each],dtype=torch.int64).to(device)\n",
        "    # torch.Size([8, 100])\n",
        "    # torch.Size([1, 100])\n",
        "    # torch.Size([1, 100])\n",
        "    print(\"Chrs\")\n",
        "    chrs = torch.zeros([8,each],dtype=torch.int64).to(device)\n",
        "    print(\"t1\")\n",
        "    t1 = torch.zeros([1,each],dtype=torch.int64).to(device)\n",
        "    print(\"t2\")\n",
        "    t2 = torch.zeros([1,each],dtype=torch.int64).to(device)# hash_dataset = hashing_algorithm(index*each, (index+1)*each, base)\n",
        "    print(\"done\")\n",
        "    # hash_dataloader = DataLoader(hash_dataset, batch_size=each, shuffle=False,collate_fn=my_collate)\n",
        "    # mp_device_loader = pl.MpDeviceLoader(hash_dataloader, device)\n",
        "    # # datak = torch.zeros()\n",
        "    # for datak,chrs,t1,t2 in mp_device_loader:\n",
        "    #     print(datak.shape)\n",
        "    #     print(chrs.shape)\n",
        "    #     print(t1.shape)\n",
        "    #     print(t2.shape)\n",
        "    # #     # print(len(loop_f))\n",
        "    # #     # data,a,b,c,d,e,f,g,h,var = loop_f\n",
        "    # #     # a = chrs[0]\n",
        "    # #     # b = chrs[1]\n",
        "    # #     # c = chrs[2]\n",
        "    # #     # d = chrs[3]\n",
        "    # #     # e = chrs[4]\n",
        "    # #     # f = chrs[5]\n",
        "    # #     # g = chrs[6]\n",
        "    # #     # h = chrs[7]\n",
        "    # #     # d = data.clone\n",
        "    # #     # print(data.device)\n",
        "    #     break\n",
        "    # s0 = torch.clone(t1)\n",
        "    # s1 = torch.clone(t2)\n",
        "    F32i = torch.LongTensor([F32])[0].to(device)\n",
        "    print(\"new\")\n",
        "    # data_n = data.clone()\n",
        "    # batch_h_n = batch_h.clone()\n",
        "    # s0,s1,t1,t2 = [var.clone() for x in range(4)]\n",
        "    a = chrs[0]\n",
        "    b = chrs[1]\n",
        "    c = chrs[2]\n",
        "    d = chrs[3]\n",
        "    e = chrs[4]\n",
        "    f = chrs[5]\n",
        "    g = chrs[6]\n",
        "    h = chrs[7]\n",
        "    data = datak#[datak[x] for x in range(len(datak))]\n",
        "    start = time.process_time()\n",
        "    for x in range(batch_n//each):\n",
        "        print(x)\n",
        "        \n",
        "        ret = SHA256_hash(data, a,b,c,d,e,f,g,h,k,t1,t2,F32i,consts)\n",
        "        # data  =torch.clone(data)\n",
        "        # chrs = torch.clone(chrs)\n",
        "        # a = chrs[0]\n",
        "        # b = chrs[1]\n",
        "        # c = chrs[2]\n",
        "        # d = chrs[3]\n",
        "        # e = chrs[4]\n",
        "        # f = chrs[5]\n",
        "        # g = chrs[6]\n",
        "        # h = chrs[7]\n",
        "        # data = [(datak)[x] for x in range(len(datak))]\n",
        "        # t1 = torch.clone(t1)\n",
        "        # t2 = torch.clone(t2)\n",
        "        # s0 = torch.clone(s0)\n",
        "        # s1 = torch.clone(s1)\n",
        "        # a = torch.clone(a)\n",
        "        # b = torch.clone(b)\n",
        "        # c = torch.clone(c)\n",
        "        # d = torch.clone(d)\n",
        "        # e = torch.clone(e)\n",
        "        # f = torch.clone(f)\n",
        "        # g = torch.clone(g)\n",
        "        # h = torch.clone(h)\n",
        "        # k = torch.clone(k)\n",
        "        # F32i = torch.clone(F32i)\n",
        "        # consts = torch.clone(consts)\n",
        "        # # print(id(data))\n",
        "        # # data = my_stack([data])\n",
        "        # # batch_h = my_stack([batch_h])\n",
        "        # # data = torch.clone(data)\n",
        "        # # chrs = torch.clone(chrs)\n",
        "        # t1 = torch.clone(t1)\n",
        "        # t2 = torch.clone(t2)\n",
        "        # print(id(data))\n",
        "start = time.process_time()\n",
        "# batch_n = 10000000\n",
        "# each = 3000000\n",
        "batches = 2\n",
        "\n",
        "each = 3000000\n",
        "batch_n = each*1\n",
        "# prc = (xmp.spawn(_mp_fn, args=(), nprocs=8,join=True))\n",
        "i = 0\n",
        "# for000000 p in prc:\n",
        "\n",
        "_mp_fn(0)\n",
        "#     i+=1\n",
        "#     print(i)\n",
        "#     p.join()0\n",
        "time_taken = time.process_time()-start\n",
        "time_taken/=batch_n\n",
        "print(batch_n)\n",
        "print(each)\n",
        "print(\"Hash Rate\",8/time_taken)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Started 0\n",
            "Datak\n",
            "Chrs\n",
            "t1\n",
            "t2\n",
            "done\n",
            "new\n",
            "0\n",
            "3000000\n",
            "3000000\n",
            "Hash Rate 94330798.03984928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFA4TxR2W_Vh"
      },
      "source": [
        "device = xm.xla_device()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZXqNFSyWDoj"
      },
      "source": [
        "a = torch.LongTensor([1,2,3,4]).to(device)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wu4gtNIXGpZ",
        "outputId": "600d5848-c6a8-46f0-8069-6f326b8fbbec"
      },
      "source": [
        "torch.div(a,2)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2500, 0.5000, 0.7500, 1.0000], device='xla:1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moMt44h9XupE",
        "outputId": "5d4fa3a3-4c92-422a-f6bd-935f234f4acb"
      },
      "source": [
        "a"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 1, 2], device='xla:1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iPa_2HP_z-O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "eabf573d-8cf8-4b12-bf4a-b24628f41da1"
      },
      "source": [
        "def _rotr(x, y):\n",
        "    return ((x >> y) | (x << (32 - y))) & F32\n",
        "\n",
        "def _maj(x, y, z):\n",
        "    return (x & y) ^ (x & z) ^ (y & z)\n",
        "\n",
        "def _ch(x, y, z):)\n",
        "        # t2 = torch.clone(t2\n",
        "    return (x & y) ^ ((~x) & z)\n",
        "\n",
        "\n",
        "def SHA256_hash(w,chrs):\n",
        "\tfor i in range(16, 64):\n",
        "\t\ts0 = _rotr(w[:,i-15], 7) ^ _rotr(w[:,i-15], 18) ^ _rotr(w[:,i-15], 3)\n",
        "\t\ts1 = _rotr(w[:,i-2], 17) ^ _rotr(w[:,i-2], 19) ^ _rotr(w[:,i-15], 10)\n",
        "\t\tw[:,i] = (w[:,i-16] + s0 + w[:,i-7] + s1) & F32\n",
        "\tfor i in range(64):\n",
        "\t\ts0 = _rotr(chrs[:,i%8], 2) ^ _rotr(chrs[:,i%8], 13) ^ _rotr(chrs[:,i%8], 22)\n",
        "\t\tt2 = s0 + _maj(chrs[:,i%8], chrs[:,(i+1)%8], chrs[:,(i+2)%8])\n",
        "\t\ts1 = _rotr(chrs[:,(i+4)%8], 6) ^ _rotr(chrs[:,(i+4)%8], 11) ^ _rotr(chrs[:,(i+4)%8], 25)\n",
        "\t\tt1 = chrs[:,(i+7)%8] + s1 + _ch(chrs[:,(i+4)%8], chrs[:,(i+5)%8], chrs[:,(i+6)%8]) + self._k[i] + w[:,i]\n",
        "\t\ti+=1\n",
        "\t\ti = i%8\n",
        "\t\tchrs[:,(i+4)%8] = (chrs[:,(i+4)%8]+t1) & F32\n",
        "\t\tchrs[:,i%8] = (t1 + t2) & F32\n",
        "\treturn chrs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-4d30942413dd>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    def _ch(x, y, z):)\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ8ke5kv-jJU"
      },
      "source": [
        "def _mp_fn(index):\n",
        "\tprint(\"Started\",index)\n",
        "\thash_dataset = hashing_algorithm(index*batch_n, (index+1)*batch_n, base)\n",
        "\thash_dataloader = DataLoader(hash_dataset, batch_size=each, shuffle=True)\n",
        "\n",
        "\tmp_device_loader = pl.MpDeviceLoader(hash_dataloader, device)\n",
        "\n",
        "\tfor data,batch_h in mp_device_loader:\n",
        "\t\tret = SHA256_hash(data, batch_h)\n",
        "\t\tprint(ret)\n",
        "_mp_fn(0)\n",
        "# xmp.spawn(_mp_fn, args=())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}